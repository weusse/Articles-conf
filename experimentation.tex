\section{Experimentation and results}\label{experimentations}
We detail and analyze in the section the results of the experimentation we performed using the six ML algorithms presented in Section \ref{ml_algorithms} over the two real datasets described in Section \ref{datasets}. We start by presenting our experimentation setting.

% experimentation setting
\subsection{Experimentation Setting}
In this section data is available for applying classification algorithm. After model creation from training data, classification operation is performed on test data. 
All the performed tests have been done in the same machine and the same operating system. To test the performance of our six chosen ML algorithms, we relied on their Python implementations available through the scikit-learn library. Scikit-learn is an open source simple and efficient tool for predictive data analysis that implements most of the existing ML algorithms

Then some of the most important performance evaluation measures like accuracy, precision, sensitivity, specificity, F-measure and area under ROC curve are evaluated and compared. 
For the details about the description of each parameter of ML we refer to the official documentation of the implementation of these algorithms in scikit-learn7. Concerning the segmentation of both datasets for the training of our ML algorithms and their testing we have considered the stratified-5-fold cross-validation in classification model construction and efficiency evaluation. This method is very useful to handle data with an unbalanced class distribution, increases the validation of classification and prevents from random and invalid results.


% Results of the experiments
\subsection{Results of the experiments}

This section presents the results of the experimentation on each real dataset for each of the six classifiers. 
\subsubsection{Decision Tree}

Table 1  below shows the performance measures (precision, recall, F measure and precision) of the results of our Decision Tree classifier after experimentation on all our datasets. The observation shows that the best scores of our classifier are achieved on the datasets DT1, DT3 and DT5 which are 97.04\%, 80.86\% and 83.41\% respectively. Also AUC (Area Under the Curve) values are higher for these same datasets which are 0.78, 0.86 and 0.76 respectively. However, we note that the sensitivity values are higher than the specificity values on the datasets DT1 and DT5 so that they are substantially identical for the datasets DT2, DT3 and DT4. This means that DT is more inclined to predict as well whether a given patient has malaria or he doesnâ€™t, on the datasets DT2, DT3 and DT4, while our classifier on the datasets DT1 and DT5 our classifier is only efficient in predicting whether a given patient has malaria. This same trend is observed on the F-scores which higher values varying between 0.91 and 0.98 on the datasets DT1, DT3 and DT5.

% Decision Tree
\begin{table}[!ht]
\centering
\begin{tabular}{*{6}{c}l r}
  \toprule
  \textbf{Datasets} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-score}&\textbf{AUC} &\textbf{Score}\\
   \midrule
  DT1 &0.97 & 1  & 0.98& 0.78&97.04 \\
  DT2 & 0.59 &0.48&0.48&0.64&63.01 \\
  DT3 &0.89 &0.85 &0.87&0.86&80.86\\
  DT4 &0.68 &0.57&0.62&0.70&65.60\\
  DT5 &0.99 &0.84&0.91&0.76&83.41\\

  
    \bottomrule
\end{tabular}
\caption{Performances measures of DT over all datasets}\label{perf-measure-dt1}
\end{table}
\subsubsection{Random Forest}
The performance of the random forest varied throughout the study depending on the dataset, although overall it performed well as shown in Table 2. Notice that best accuracy are achieved by random forest classifier on the datasets DT1, DT2 and DT5 which are respectively 97.13\%, 80.86\% and 78.35\%. In contrast with the results obtained with the DT classifier, the Sensivity values are higher than specificity values on datasets DT1 and DT5 whereas the inverse is noticed on the dataset DT3. At the same time we note that these values are roughly identical on the datasets DT3 and D4
%Random Forest
\begin{table}[!ht]
\centering
\begin{tabular}{*{6}{c}l r}
  \toprule
  \textbf{Datasets} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-score}&\textbf{AUC} &\textbf{Score}\\
   \midrule
  DT1 &0.97 &1   &0.99 &0.81 &97.13 \\
  DT2 &0.63  & 0.34  &0.44&0.64&63.33 \\
  DT3 &0.89 &0.85 &0.87&087&80.86\\
  DT4 &0.68 &0.56&0.62&0.70&65.82\\
  DT5 &0.99 &0.84&0.91&0.76&78.35\\
  
  
    \bottomrule
\end{tabular}
\caption{Performances measures of RF over all datasets}\label{perf-measure-dt1}
\end{table}


%Logistic Regression
\subsubsection{Logistic Regression}

Logistic regression In table 3 we show the performance measures LR classifier experimented on our five datasets. We notice that our classifier have overall precision which are vary between 58\% and 98\%. 

\begin{table}[!ht]
\centering
\begin{tabular}{*{6}{c}l r}
  \toprule
  \textbf{Datasets} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-score}&\textbf{AUC} &\textbf{Score}\\
   \midrule
  DT1 &0.97 &1   &0.99 &0.79 &97.19 \\
  DT2 & 0.58 &0.36   &0.44&0.63&61.96\\
  DT3 &0.85 &0.88 &0.86&0.86&79.59\\
  DT4 &0.98 &0.56&0.92&0.70&65.82\\
  DT5 & 0.90&0.78&0.88&0.84&81.86\\
  
  
    \bottomrule
\end{tabular}
\caption{Performances measures of LR over all datasets}\label{perf-measure-dt1}
\end{table}
We observe that the higher precision is obtained with DT4 dataset while the corresponding score is equal to 65.82\% is the lowest of all other datasets. Also we notice that the LR presents homogeneous results on the DT3 dataset with an accuracy of 85\%, a sensitivity equal to 88\%, an F-score of 92\%, an AUC which is 0.86 and a score equal to 79.59\%. . We also note that the best AUC and the best F-score are obtained by LR on the DT3 dataset.
\subsubsection{Naives Bayes}
In contrast with the results above, NB classifier presents very heterogeneous performances regarding the performance measures used. In fact, we observe that the best precision is achieved on the dataset DT5 which is 99\%, although the best F-score and the higher accuracy are obtained on the dataset DT1 which are 0.99 and 97.13\% respectively and finally the best AUC is observed on the dataset DT3 which is 0.85. We also note that the best specificity is obtained on DT4 and varies between 0.65 and 0.70 (see appendices).
\begin{table}[!ht]
\centering
\begin{tabular}{*{6}{c}l r}
  \toprule
  \textbf{Datasets} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-score}&\textbf{AUC} &\textbf{Score}\\
   \midrule
  DT1 &0.97 &1   &0.99 &0.81 &97.13 \\
  DT2 & 0.60 &0.34   &0.43&0.63&62.86 \\
  DT3 &0.86 &0.87 &0.86&0.85&79.94\\
  DT4 &0.68 &0.59&0.63&0.70&65.63\\
  DT5 &0.99 &0.82&0.90&0.84&85.61\\
  
  
    \bottomrule
\end{tabular}
\caption{Performances measures of NB over all datasets}\label{perf-measure-dt1}
\end{table}

%SVM
\begin{center}
\textbf{SVM}

\end{center}
\begin{table}[!ht]
\centering
\begin{tabular}{*{6}{c}l r}
  \toprule
  \textbf{Datasets} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-score}&\textbf{AUC} &\textbf{Score}\\
   \midrule
  DT1 &0.97 &1   &0.99 &.52 &97.13 \\
  DT2 &0.58  &0.05   & 0.09&0.62&62.86\\
  DT3 &0.57 & 0.86&0.86&0.85&79.94\\
  DT4 & 0.68&0.58&0.62&0.69&65.63\\
  DT5 &0.99 &0.86&0.92&0.80&85.61\\
    \bottomrule
\end{tabular}
\caption{Performances measures of SVM over all datasets}\label{perf-measure-dt1}
\end{table}
%ANN
\begin{center}
\textbf{ANN}
\end{center}
\begin{table}[!ht]
\centering
\begin{tabular}{*{6}{c}l r}
  \toprule
  \textbf{Datasets} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-score}&\textbf{AUC} &\textbf{Score}\\
   \midrule
  DT1 0.97&1 &0.99   &0.84 &97.15  \\
  DT2 &0.59  &0.40   &0.48&0.65&62.86 \\
  DT3 &0.89 &0.85 &0.87&0.87&86.68\\
  DT4 &0.68 &0.58&0.62&0.70&0.70\\
  DT5 &0.99 &0.84&0.91&0.79&83.26\\ 
    \bottomrule
\end{tabular}
\caption{Performances measures of ANN over all datasets}\label{perf-measure-dt1}
\end{table}

\subsubsection*{\bf Experiments with DT1.}
